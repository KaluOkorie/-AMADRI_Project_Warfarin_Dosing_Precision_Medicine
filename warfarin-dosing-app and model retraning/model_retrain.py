# -*- coding: utf-8 -*-
"""Model_retrain

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AL7jC30cak69zWAV-q1GXhkkB7GsIhsD
"""

#!/usr/bin/env python3
"""
Warfarin Dosing Model Retraining Script

"""

import pandas as pd
import numpy as np
import joblib
import xgboost as xgb
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# 1. LOAD DATA
# ============================================================================
print("Loading data...")
X_train = pd.read_csv("X_train.csv")
X_val = pd.read_csv("X_val.csv")
X_test = pd.read_csv("X_test.csv")
y_train = pd.read_csv("y_train.csv").squeeze()
y_val = pd.read_csv("y_val.csv").squeeze()
y_test = pd.read_csv("y_test.csv").squeeze()

print(f"X_train shape: {X_train.shape}")
print(f"X_val shape: {X_val.shape}")
print(f"X_test shape: {X_test.shape}")

# ============================================================================
# 2. FEATURE ENGINEERING (EXACTLY as in original)
# ============================================================================
def engineer_features(df):
    df = df.copy()

    # CYP2C9 encoding
    cyp2c9_activity = {
        "*1/*1": 2.0, "*1/*2": 1.5, "*1/*3": 1.0,
        "*2/*2": 1.0, "*2/*3": 0.5, "*3/*3": 0.0
    }
    cyp2c9_pheno = {
        "*1/*1": "Extensive", "*1/*2": "Intermediate",
        "*1/*3": "Intermediate", "*2/*2": "Poor",
        "*2/*3": "Poor", "*3/*3": "Poor"
    }

    if "CYP2C9" in df.columns:
        df["CYP2C9_Phenotype"] = df["CYP2C9"].map(cyp2c9_pheno).fillna("Intermediate")
        df["CYP2C9_Activity"] = df["CYP2C9"].map(cyp2c9_activity).fillna(1.5)

    # VKORC1 encoding
    vkorc1_sens = {"G/G": 0.0, "A/G": 1.0, "A/A": 2.0}
    vkorc1_pheno = {"G/G": "Normal", "A/G": "Intermediate", "A/A": "Sensitive"}

    if "VKORC1" in df.columns:
        df["VKORC1_Phenotype"] = df["VKORC1"].map(vkorc1_pheno).fillna("Intermediate")
        df["VKORC1_Sensitivity"] = df["VKORC1"].map(vkorc1_sens).fillna(1.0)

    # CYP4F2 encoding
    cyp4f2_score = {"C/C": 0.0, "C/T": 1.0, "T/T": 2.0}
    cyp4f2_geno = {"C/C": "Normal", "C/T": "Heterozygous", "T/T": "Variant"}

    if "CYP4F2" in df.columns:
        df["CYP4F2_Genotype"] = df["CYP4F2"].map(cyp4f2_geno).fillna("Normal")
        df["CYP4F2_Score"] = df["CYP4F2"].map(cyp4f2_score).fillna(0.0)

    # BSA and BMI
    if {"Height_cm", "Weight_kg"}.issubset(df.columns):
        df["BSA"] = 0.007184 * (df["Height_cm"] ** 0.725) * (df["Weight_kg"] ** 0.425)
        df["BMI"] = df["Weight_kg"] / ((df["Height_cm"] / 100) ** 2)
        df["BMI_Category"] = pd.cut(
            df["BMI"],
            bins=[0, 18.5, 25, 30, 100],
            labels=["Underweight", "Normal", "Overweight", "Obese"]
        )

    # eGFR (simplified calculation)
    if {"Age", "Weight_kg", "Sex"}.issubset(df.columns):
        sex_factor = np.where(df["Sex"] == "F", 0.85, 1.0)
        df["eGFR"] = ((140 - df["Age"]) * df["Weight_kg"] * sex_factor) / (72 * 1.2)
        df["Renal_Impairment"] = pd.cut(
            df["eGFR"],
            bins=[0, 30, 60, 90, 200],
            labels=["Severe", "Moderate", "Mild", "Normal"]
        )

    # Interaction score
    interaction_drugs = ["Amiodarone", "Antibiotics", "Aspirin", "Statins"]
    present = [c for c in interaction_drugs if c in df.columns]
    if present:
        df["Interaction_Score"] = df[present].sum(axis=1)

    # Comorbidity score
    comorb = ["Hypertension", "Diabetes", "Chronic_Kidney_Disease", "Heart_Failure"]
    present = [c for c in comorb if c in df.columns]
    if present:
        df["Comorbidity_Score"] = df[present].sum(axis=1)

    # Genetic burden
    components = []
    if "CYP2C9_Activity" in df.columns:
        components.append((2 - df["CYP2C9_Activity"]) / 2)
    if "VKORC1_Sensitivity" in df.columns:
        components.append(df["VKORC1_Sensitivity"] / 2)
    if "CYP4F2_Score" in df.columns:
        components.append(df["CYP4F2_Score"] / 2)

    if components:
        df["Genetic_Burden_Score"] = np.mean(components, axis=0)

    return df

print("Engineering features...")
X_train_fe = engineer_features(X_train)
X_val_fe = engineer_features(X_val)
X_test_fe = engineer_features(X_test)

# ============================================================================
# 3. PREPROCESSING
# ============================================================================
print("Preprocessing data...")

# Identify numeric and categorical features
numeric_features = [
    "Age", "Weight_kg", "Height_cm", "BSA", "BMI", "eGFR",
    "CYP2C9_Activity", "VKORC1_Sensitivity", "CYP4F2_Score",
    "Genetic_Burden_Score", "Interaction_Score", "Comorbidity_Score"
]

# Add binary features if they exist
binary_features = [
    "Hypertension", "Diabetes", "Chronic_Kidney_Disease",
    "Heart_Failure", "Amiodarone", "Antibiotics", "Aspirin", "Statins"
]
binary_features = [c for c in binary_features if c in X_train_fe.columns]
numeric_features.extend(binary_features)

categorical_features = [
    "Sex", "Ethnicity", "CYP2C9_Phenotype", "VKORC1_Phenotype",
    "CYP4F2_Genotype", "BMI_Category", "Renal_Impairment"
]

# Keep only columns that exist
numeric_features = [c for c in numeric_features if c in X_train_fe.columns]
categorical_features = [c for c in categorical_features if c in X_train_fe.columns]

# Fill missing values
for df in (X_train_fe, X_val_fe, X_test_fe):
    for col in numeric_features:
        if df[col].isna().any():
            df[col] = df[col].fillna(X_train_fe[col].median())

    for col in categorical_features:
        if df[col].isna().any():
            df[col] = df[col].fillna(X_train_fe[col].mode()[0])

# Build preprocessing pipeline
preprocessor = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(), numeric_features),
        ("cat", OneHotEncoder(drop="first", sparse_output=False, handle_unknown="ignore"), categorical_features),
    ]
)

# Fit and transform
X_train_proc = preprocessor.fit_transform(X_train_fe)
X_val_proc = preprocessor.transform(X_val_fe)
X_test_proc = preprocessor.transform(X_test_fe)

# Get feature names
num_cols = numeric_features
cat_cols = list(preprocessor.named_transformers_["cat"].get_feature_names_out(categorical_features))
all_cols = num_cols + cat_cols

X_train_proc = pd.DataFrame(X_train_proc, columns=all_cols)
X_val_proc = pd.DataFrame(X_val_proc, columns=all_cols)
X_test_proc = pd.DataFrame(X_test_proc, columns=all_cols)

print(f"Processed features: {len(all_cols)}")

# ============================================================================
# 4. ADVANCED XGBOOST TUNING
# ============================================================================
print("Training XGBoost model...")

# Define search space
param_dist = {
    "n_estimators": [200, 300, 400, 500],
    "max_depth": [3, 4, 5, 6, 7],
    "learning_rate": [0.01, 0.05, 0.1, 0.2],
    "subsample": [0.7, 0.8, 0.9, 1.0],
    "colsample_bytree": [0.7, 0.8, 0.9, 1.0],
    "gamma": [0, 0.1, 0.2, 0.3],
    "reg_alpha": [0, 0.01, 0.1, 1],
    "reg_lambda": [0, 0.01, 0.1, 1],
    "min_child_weight": [1, 3, 5, 7]
}

# Use RandomizedSearchCV for efficiency
xgb_base = xgb.XGBRegressor(random_state=42, n_jobs=-1, tree_method="hist")

random_search = RandomizedSearchCV(
    estimator=xgb_base,
    param_distributions=param_dist,
    n_iter=30,
    scoring="neg_mean_squared_error",
    cv=3,
    verbose=1,
    random_state=42,
    n_jobs=-1
)

# Train on combined training and validation for final model
X_combined = pd.concat([X_train_proc, X_val_proc])
y_combined = pd.concat([y_train, y_val])

random_search.fit(X_combined, y_combined)

# Get best model
best_model = random_search.best_estimator_
best_params = random_search.best_params_

print(f"\nBest parameters: {best_params}")

# ============================================================================
# 5. EVALUATION
# ============================================================================
print("\nEvaluating model...")

# Predictions
y_train_pred = best_model.predict(X_train_proc)
y_val_pred = best_model.predict(X_val_proc)
y_test_pred = best_model.predict(X_test_proc)

# Calculate metrics
def calculate_metrics(y_true, y_pred, set_name):
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    within_1mg = (np.abs(y_pred - y_true) <= 1.0).mean() * 100
    within_2mg = (np.abs(y_pred - y_true) <= 2.0).mean() * 100

    return {
        "Set": set_name,
        "RMSE": rmse,
        "MAE": mae,
        "R2": r2,
        "Within ±1mg (%)": within_1mg,
        "Within ±2mg (%)": within_2mg
    }

results = []
results.append(calculate_metrics(y_train, y_train_pred, "Train"))
results.append(calculate_metrics(y_val, y_val_pred, "Val"))
results.append(calculate_metrics(y_test, y_test_pred, "Test"))

results_df = pd.DataFrame(results)
print("\n" + "="*50)
print("MODEL PERFORMANCE")
print("="*50)
print(results_df.to_string(index=False))

# ============================================================================
# 6. SAVE ONLY NECESSARY FILES FOR HUGGING FACE
# ============================================================================
print("\nSaving files for Hugging Face deployment...")

# Save the model
joblib.dump(best_model, "final_warfarin_model_xgboost.pkl")
print("✓ Saved: final_warfarin_model_xgboost.pkl")

# Save the preprocessor
joblib.dump(preprocessor, "preprocessor.pkl")
print("✓ Saved: preprocessor.pkl")

# Save feature names for reference
feature_info = {
    "feature_names_in": preprocessor.feature_names_in_,
    "feature_names_out": all_cols,
    "numeric_features": numeric_features,
    "categorical_features": categorical_features
}
joblib.dump(feature_info, "feature_info.pkl")
print("✓ Saved: feature_info.pkl")

# Save a small sample for testing
sample_data = {
    "X_sample": X_test_proc.iloc[:5].values.tolist(),
    "y_sample": y_test.iloc[:5].values.tolist(),
    "columns": all_cols
}
import json
with open("sample_data.json", "w") as f:
    json.dump(sample_data, f)
print("✓ Saved: sample_data.json")

print("\n" + "="*50)
print("RETRAINING COMPLETE!")
print(f"Test Set Performance:")
print(f"  - RMSE: {results_df.loc[results_df['Set']=='Test', 'RMSE'].values[0]:.3f} mg")
print(f"  - MAE: {results_df.loc[results_df['Set']=='Test', 'MAE'].values[0]:.3f} mg")
print(f"  - R²: {results_df.loc[results_df['Set']=='Test', 'R2'].values[0]:.3f}")
print(f"  - Within ±1mg: {results_df.loc[results_df['Set']=='Test', 'Within ±1mg (%)'].values[0]:.1f}%")
print("="*50)

