# -*- coding: utf-8 -*-
"""Day 2 Data Preparation & EDA.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15ocSCIJsfLYvy-11XdBkLHks1c9k_rJj

# Day 2 Data Preparation & EDA

### Importing Library & Data Loading
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

# Set display options
pd.set_option('display.max_columns', 100)
pd.set_option('display.max_rows', 100)

"""### creates essential train/val/test splits"""

class MinimalWarfarinDataPreparer:

    def __init__(self):
        # Configuration
        self.input_file = 'combined_clean.csv'
        self.target_col = 'Final_Stable_Dose_mg'
        self.id_col = 'Patient_ID'

        # Data leakage variables to remove
        self.leakage_vars = [
            'TTR_pct',                     # Therapeutic outcome
            'INR_Stabilization_Days',      # Treatment duration outcome
            'Adverse_Event',               # Safety outcome
            'Alcohol_Score',               # Derived variable
            'VitK_Score',                  # Derived variable
            'On_Amiodarone'                # Perfectly correlated with Amiodarone
        ]

        # Output file names (simple and consistent)
        self.X_train_file = 'X_train.csv'
        self.X_val_file = 'X_val.csv'
        self.X_test_file = 'X_test.csv'
        self.y_train_file = 'y_train.csv'
        self.y_val_file = 'y_val.csv'
        self.y_test_file = 'y_test.csv'

        # Split configuration
        self.test_size = 0.2
        self.val_size = 0.2
        self.random_state = 42

    def run(self):
        """Run the minimal preparation pipeline"""
        print("=" * 70)
        print("DAY 2: MINIMAL DATA PREPARATION")
        print("=" * 70)

        # Step 1: Load data
        print("\nSTEP 1: Loading merged dataset...")
        df = pd.read_csv(self.input_file)
        print(f"Loaded: {df.shape[0]} rows, {df.shape[1]} columns")

        # Step 2: Remove leakage variables
        print("\nSTEP 2: Removing leakage variables...")
        vars_to_remove = [v for v in self.leakage_vars if v in df.columns]
        df = df.drop(columns=vars_to_remove, errors='ignore')
        print(f"Removed {len(vars_to_remove)} leakage variables")

        # Step 3: Check for missing values
        print("\nSTEP 3: Checking missing values...")
        missing_summary = df.isnull().sum()
        missing_vars = missing_summary[missing_summary > 0]
        if len(missing_vars) > 0:
            print(f"Variables with missing values: {list(missing_vars.index)}")
            for var, count in missing_vars.items():
                pct = count / len(df) * 100
                print(f"  {var}: {count} missing ({pct:.1f}%)")
        else:
            print("No missing values found")

        # Step 4: Separate features and target
        print("\nSTEP 4: Separating features and target...")
        X = df.drop(columns=[self.target_col, self.id_col], errors='ignore')
        y = df[self.target_col]
        print(f"Features: {X.shape[1]} columns")
        print(f"Target: {y.name} (range: {y.min():.1f}-{y.max():.1f} mg)")

        # Step 5: Split data
        print("\nSTEP 5: Splitting data (60-20-20)...")

        # First split: Train+Val vs Test
        X_temp, X_test, y_temp, y_test = train_test_split(
            X, y,
            test_size=self.test_size,
            random_state=self.random_state,
            shuffle=True
        )

        # Second split: Train vs Validation
        val_ratio = self.val_size / (1 - self.test_size)
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp,
            test_size=val_ratio,
            random_state=self.random_state,
            shuffle=True
        )

        # Print split statistics
        print(f"  Training set: {X_train.shape[0]} samples")
        print(f"  Validation set: {X_val.shape[0]} samples")
        print(f"  Test set: {X_test.shape[0]} samples")

        # Check distribution consistency
        dose_means = [y_train.mean(), y_val.mean(), y_test.mean()]
        mean_cv = np.std(dose_means) / np.mean(dose_means)
        print(f"  Mean dose consistency: CV = {mean_cv:.3f}")

        # Step 6: Save files
        print("\nSTEP 6: Saving essential files...")

        # Save features
        X_train.to_csv(self.X_train_file, index=False)
        X_val.to_csv(self.X_val_file, index=False)
        X_test.to_csv(self.X_test_file, index=False)

        # Save targets
        y_train.to_csv(self.y_train_file, index=False, header=True)
        y_val.to_csv(self.y_val_file, index=False, header=True)
        y_test.to_csv(self.y_test_file, index=False, header=True)

        print(f"  Saved: {self.X_train_file}")
        print(f"  Saved: {self.X_val_file}")
        print(f"  Saved: {self.X_test_file}")
        print(f"  Saved: {self.y_train_file}")
        print(f"  Saved: {self.y_val_file}")
        print(f"  Saved: {self.y_test_file}")

        # Step 7: Quick EDA summary
        print("\n" + "=" * 70)
        print("DATA SUMMARY:")
        print("=" * 70)
        print(f"1. Original dataset: {df.shape[0]} patients")
        print(f"2. Features after cleanup: {X_train.shape[1]} variables")
        print(f"3. Target variable: {self.target_col}")
        print(f"   - Mean dose: {y.mean():.2f} mg")
        print(f"   - Dose range: {y.min():.1f} - {y.max():.1f} mg")
        print(f"   - Dose categories:")
        print(f"     Low (<2.5 mg): {(y < 2.5).sum()} patients")
        print(f"     Medium (2.5-5 mg): {((y >= 2.5) & (y <= 5)).sum()} patients")
        print(f"     High (>5 mg): {(y > 5).sum()} patients")
        print(f"4. Data splits:")
        print(f"   Train: {X_train.shape[0]} samples (60%)")
        print(f"   Validation: {X_val.shape[0]} samples (20%)")
        print(f"   Test: {X_test.shape[0]} samples (20%)")

        # Return data for immediate use
        return X_train, X_val, X_test, y_train, y_val, y_test

"""### EXECUTION"""

if __name__ == "__main__":
    # Run the minimal pipeline
    preparer = MinimalWarfarinDataPreparer()
    X_train, X_val, X_test, y_train, y_val, y_test = preparer.run()

    print("\n" + "=" * 70)
    print("FILES CREATED:")
    print("=" * 70)
    print("X_train.csv - Training features")
    print("X_val.csv   - Validation features")
    print("X_test.csv  - Test features")
    print("y_train.csv - Training target values")
    print("y_val.csv   - Validation target values")
    print("y_test.csv  - Test target values")
    print("=" * 70)

"""# VISUALIZATION SECTION"""

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

class WarfarinVisualizer:
    def __init__(self, df, target_col='Final_Stable_Dose_mg'):
        self.df = df
        self.target_col = target_col

    def plot_correlation_heatmap(self):
        plt.figure(figsize=(10,8))
        corr = self.df.select_dtypes(include='number').corr()
        sns.heatmap(corr, cmap="coolwarm", center=0, annot=False)
        plt.title("Feature Correlation Heatmap")
        plt.show()

    def plot_target_distribution(self):
        plt.figure(figsize=(6,4))
        sns.histplot(self.df[self.target_col], bins=30, kde=True, color='steelblue')
        plt.title("Stable Dose Distribution")
        plt.xlabel("Dose (mg)")
        plt.ylabel("Count")
        plt.show()

    def plot_numeric_distributions(self, cols=['Age','Weight_kg','INR_Stabilization_Days']):
        for col in cols:
            if col in self.df.columns:
                plt.figure(figsize=(6,4))
                sns.histplot(self.df[col], bins=30, kde=True, color='darkgreen')
                plt.title(f"Distribution of {col}")
                plt.xlabel(col)
                plt.ylabel("Count")
                plt.show()

    def plot_outlier_boxplot(self):
        plt.figure(figsize=(6,4))
        sns.boxplot(x=self.df[self.target_col], color='tomato')
        plt.title("Outlier Detection: Stable Dose")
        plt.xlabel("Dose (mg)")
        plt.show()

### EXECUTION FOR VISUALIZATION
if __name__ == "__main__":
    df_clean = pd.read_csv("combined_clean.csv")
    viz = WarfarinVisualizer(df_clean)

    viz.plot_correlation_heatmap()
    viz.plot_target_distribution()
    viz.plot_numeric_distributions()
    viz.plot_outlier_boxplot()