# -*- coding: utf-8 -*-
"""Day 2_EDA.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dHIPLPf2N4C7-QPyUqIAgAgSFV750Hk0

# Day 2 EDA

## Importing Library & Data Loading
"""

# Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

# Set display options
pd.set_option('display.max_columns', 100)
pd.set_option('display.max_rows', 100)

# Load the dataset
df = pd.read_csv('combined_clean.csv')   #description: read the CSV file into a pandas DataFrame
print(f"Dataset shape: {df.shape}")   #description: display number of rows and columns

"""## Basic Data Assessment"""

# Display first few rows
print("\nFirst 5 rows:")
print(df.head())

# Check data types
print("\nData types distribution:")
print(df.dtypes.value_counts())

# Check for duplicates
duplicates = df.duplicated().sum()
print(f"\nDuplicate rows: {duplicates}")

# Get summary statistics
print("\nSummary statistics for key numerical variables:")
key_numerical = ['Final_Stable_Dose_mg', 'Age', 'Weight_kg', 'TTR_pct', 'INR_Stabilization_Days']
if all(col in df.columns for col in key_numerical):
    print(df[key_numerical].describe().round(2))

"""## possible data leakage
- Post-treatment fallacy": Including outcomes as predictors creates impossible causal relationships.

- Data leakage: Model learns from information that wouldn't be available at prediction time


"""

leakage_variables = [
    'TTR_pct',                    # Therapeutic outcome - can't predict this before treatment
    'INR_Stabilization_Days',     # Post-treatment outcome - known only after treatment
    'Adverse_Event',              # Safety outcome - known only after treatment
    'Alcohol_Score',              # Derived from Alcohol_Intake - redundant
    'VitK_Score'                  # Derived from Diet_VitK_Intake - redundant
]

"""## Missing Values"""

print("\n STRATEGIC MISSING VALUE ANALYSIS ===")

# Focus on predictors only - exclude leakage variables
predictor_columns = [col for col in df.columns if col not in leakage_variables]

# Calculate missingness only for predictors
missing_data = df[predictor_columns].isnull().sum()
missing_percentage = (missing_data / len(df)) * 100

# Create clinical missingness summary
missing_summary = pd.DataFrame({
    'Missing_Count': missing_data,
    'Missing_Percentage': missing_percentage.round(1),
    'Clinical_Implication': ''
})

# Add clinical implications
clinical_implications = {
    'Adverse_Event': 'Outcome variable - missing = no event (impute as "No")',
    'Alcohol_Intake': 'Baseline data - missing may indicate non-disclosure',
    'Alcohol_Score': 'Derived variable - can be recalculated if needed',
    'VitK_Score': 'Derived variable - can be recalculated if needed'
}

for col, implication in clinical_implications.items():
    if col in missing_summary.index:
        missing_summary.loc[col, 'Clinical_Implication'] = implication

# Filter for columns with missingness
missing_summary = missing_summary[missing_summary['Missing_Count'] > 0]
print("\nMissing Value Analysis (Predictors Only):")
print("-" * 50)

# Display with clinical context
for idx, row in missing_summary.iterrows():
    print(f"\n{idx}:")
    print(f"  Missing: {row['Missing_Count']:,} ({row['Missing_Percentage']}%)")
    print(f"  Implication: {row['Clinical_Implication']}")

# Decision: How to handle each missingness
print("\n" + "="*50)
print("FOR FEATURE ENGINEERING DECISIONS:")
print("="*50)
print("""
1. Adverse_Event: Exclude from features (it's an outcome)
2. Alcohol_Intake: Keep as-is, create 'unknown' category
3. Alcohol_Score: Recalculate from Alcohol_Intake if possible
4. VitK_Score: Recalculate from Diet_VitK_Intake

RATIONALE:
- We cannot use outcomes to predict outcomes (circular reasoning)
- Missing baseline data should be preserved as separate category
- Derived scores should be re-derived from original variables
""")

"""## Variable Distribution Analysis"""

print("\n===  PREDICTOR VARIABLE DISTRIBUTION ANALYSIS ===")
# Create predictor-only dataframe
predictor_df = df.drop(columns=leakage_variables, errors='ignore')
print(f"Predictor dataset: {predictor_df.shape[0]:,} patients, {predictor_df.shape[1]} predictors")
print(f"Target variable: 'Final_Stable_Dose_mg'")

# Separate variable types
categorical_predictors = predictor_df.select_dtypes(include=['object']).columns.tolist()
numerical_predictors = predictor_df.select_dtypes(include=[np.number]).columns.tolist()

# Remove Patient_ID from categorical (it's an identifier, not a feature)
if 'Patient_ID' in categorical_predictors:
    categorical_predictors.remove('Patient_ID')

# Remove target from numerical predictors
if 'Final_Stable_Dose_mg' in numerical_predictors:
    numerical_predictors.remove('Final_Stable_Dose_mg')

print(f"\nCategorical predictors ({len(categorical_predictors)}):")
for col in categorical_predictors:
    unique_count = predictor_df[col].nunique()
    print(f"  {col}: {unique_count} categories")

print(f"\nNumerical predictors ({len(numerical_predictors)}):")
print("  " + ", ".join(numerical_predictors[:10]) + ("..." if len(numerical_predictors) > 10 else ""))

print("\n" + "="*60)
print("ANALYSIS 1: GENETIC VARIANT DISTRIBUTIONS")
print("="*60)

# Analyze genetic variants
genetic_variants = ['CYP2C9', 'VKORC1', 'CYP4F2', 'CYP2C9_pcyp3', 'VKORC1_AA']

for gene in genetic_variants:
    if gene in predictor_df.columns:
        print(f"\n{gene}:")
        counts = predictor_df[gene].value_counts()
        percentages = (counts / len(predictor_df) * 100).round(1)

        for value, (count, pct) in enumerate(zip(counts.values, percentages.values)):
            print(f"  {counts.index[value]}: {count:,} ({pct}%)")

        # Check for missing values in genetic data
        missing = predictor_df[gene].isnull().sum()
        if missing > 0:
            print(f"  Missing: {missing:,} ({missing/len(predictor_df)*100:.1f}%)")

print("\n" + "="*60)
print("ANALYSIS 2: CLINICAL VARIABLE DISTRIBUTIONS")
print("="*60)

# Key clinical variables
clinical_vars = ['Age', 'Weight_kg', 'Height_cm', 'BMI', 'Hypertension', 'Diabetes',
                 'Chronic_Kidney_Disease', 'Heart_Failure', 'Amiodarone', 'Antibiotics',
                 'Aspirin', 'Statins']

# Calculate BMI if not present
if 'Height_cm' in predictor_df.columns and 'Weight_kg' in predictor_df.columns:
    predictor_df['BMI'] = predictor_df['Weight_kg'] / ((predictor_df['Height_cm'] / 100) ** 2)

# Check which clinical variables exist in our dataset
available_clinical = [var for var in clinical_vars if var in predictor_df.columns]

for var in available_clinical:
    if predictor_df[var].dtype in [np.int64, np.float64]:
        # Numerical variable
        stats = predictor_df[var].describe()
        print(f"\n{var} (continuous):")
        print(f"  Mean: {stats['mean']:.1f}, Std: {stats['std']:.1f}")
        print(f"  Min: {stats['min']:.1f}, 25th: {stats['25%']:.1f}, Median: {stats['50%']:.1f}")
        print(f"  75th: {stats['75%']:.1f}, Max: {stats['max']:.1f}")

        # Check for outliers or biologically implausible values
        if var == 'Age':
            under_18 = (predictor_df[var] < 18).sum()
            over_100 = (predictor_df[var] > 100).sum()
            if under_18 > 0 or over_100 > 0:
                print(f"  Age outliers: {under_18} <18y, {over_100} >100y")

        elif var == 'Weight_kg':
            under_30 = (predictor_df[var] < 30).sum()
            over_200 = (predictor_df[var] > 200).sum()
            if under_30 > 0 or over_200 > 0:
                print(f" Weight outliers: {under_30} <30kg, {over_200} >200kg")

        elif var == 'BMI':
            # Classify BMI categories
            bmi_cats = pd.cut(predictor_df['BMI'],
                             bins=[0, 18.5, 25, 30, 35, 40, 100],
                             labels=['Underweight', 'Normal', 'Overweight',
                                     'Obese I', 'Obese II', 'Obese III'])
            print(f"  BMI Categories:")
            for cat in bmi_cats.cat.categories:
                count = (bmi_cats == cat).sum()
                if count > 0:
                    pct = count / len(predictor_df) * 100
                    print(f"    {cat}: {count:,} ({pct:.1f}%)")

    else:
        # Binary or categorical clinical variable
        counts = predictor_df[var].value_counts()
        total = len(predictor_df)
        print(f"\n{var} (categorical):")
        for value, count in counts.items():
            pct = count / total * 100
            print(f"  {value}: {count:,} ({pct:.1f}%)")

print("\n" + "="*60)
print("ANALYSIS 3: LIFESTYLE VARIABLE DISTRIBUTIONS")
print("="*60)

# Lifestyle variables
lifestyle_vars = ['Alcohol_Intake', 'Smoking_Status', 'Diet_VitK_Intake']

for var in lifestyle_vars:
    if var in predictor_df.columns:
        counts = predictor_df[var].value_counts(dropna=False)
        total = len(predictor_df)
        print(f"\n{var}:")

        for value, count in counts.items():
            pct = count / total * 100
            label = 'Missing' if pd.isna(value) else value
            print(f"  {label}: {count:,} ({pct:.1f}%)")

print("\n" + "="*60)
print("ANALYSIS 4: VISUAL DISTRIBUTION CHECK")
print("="*60)

# Create visualizations for key distributions
fig, axes = plt.subplots(3, 3, figsize=(15, 12))
axes = axes.flatten()

# Plot 1: Warfarin dose distribution (target variable)
if 'Final_Stable_Dose_mg' in df.columns:
    axes[0].hist(df['Final_Stable_Dose_mg'], bins=30, edgecolor='black', alpha=0.7, color='steelblue')
    axes[0].axvline(df['Final_Stable_Dose_mg'].mean(), color='red', linestyle='--', label=f'Mean: {df["Final_Stable_Dose_mg"].mean():.2f}')
    axes[0].axvline(df['Final_Stable_Dose_mg'].median(), color='green', linestyle='--', label=f'Median: {df["Final_Stable_Dose_mg"].median():.2f}')
    axes[0].set_title('Target: Warfarin Dose Distribution', fontsize=12, fontweight='bold')
    axes[0].set_xlabel('Daily Dose (mg)')
    axes[0].set_ylabel('Frequency')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)

# Plot 2: Age distribution
if 'Age' in predictor_df.columns:
    axes[1].hist(predictor_df['Age'], bins=30, edgecolor='black', alpha=0.7, color='orange')
    axes[1].set_title('Age Distribution', fontsize=12, fontweight='bold')
    axes[1].set_xlabel('Age (years)')
    axes[1].set_ylabel('Frequency')
    axes[1].grid(True, alpha=0.3)

# Plot 3: Weight distribution
if 'Weight_kg' in predictor_df.columns:
    axes[2].hist(predictor_df['Weight_kg'], bins=30, edgecolor='black', alpha=0.7, color='green')
    axes[2].set_title('Weight Distribution', fontsize=12, fontweight='bold')
    axes[2].set_xlabel('Weight (kg)')
    axes[2].set_ylabel('Frequency')
    axes[2].grid(True, alpha=0.3)

# Plot 4: CYP2C9 genotype distribution
if 'CYP2C9' in predictor_df.columns:
    cyp_counts = predictor_df['CYP2C9'].value_counts().sort_index()
    bars = axes[3].bar(range(len(cyp_counts)), cyp_counts.values, color='purple', alpha=0.7)
    axes[3].set_title('CYP2C9 Genotype Distribution', fontsize=12, fontweight='bold')
    axes[3].set_xlabel('Genotype')
    axes[3].set_ylabel('Count')
    axes[3].set_xticks(range(len(cyp_counts)))
    axes[3].set_xticklabels(cyp_counts.index, rotation=45, ha='right')

    # Add counts on bars
    for bar, count in zip(bars, cyp_counts.values):
        height = bar.get_height()
        axes[3].text(bar.get_x() + bar.get_width()/2, height,
                    f'{count:,}', ha='center', va='bottom', fontsize=9)

# Plot 5: VKORC1 genotype distribution
if 'VKORC1' in predictor_df.columns:
    vkor_counts = predictor_df['VKORC1'].value_counts().sort_index()
    bars = axes[4].bar(range(len(vkor_counts)), vkor_counts.values, color='brown', alpha=0.7)
    axes[4].set_title('VKORC1 Genotype Distribution', fontsize=12, fontweight='bold')
    axes[4].set_xlabel('Genotype')
    axes[4].set_ylabel('Count')
    axes[4].set_xticks(range(len(vkor_counts)))
    axes[4].set_xticklabels(vkor_counts.index, rotation=45, ha='right')

    for bar, count in zip(bars, vkor_counts.values):
        height = bar.get_height()
        axes[4].text(bar.get_x() + bar.get_width()/2, height,
                    f'{count:,}', ha='center', va='bottom', fontsize=9)

# Plot 6: Alcohol intake (with missing shown)
if 'Alcohol_Intake' in predictor_df.columns:
    # Fill NaN with 'Missing' for visualization
    alcohol_data = predictor_df['Alcohol_Intake'].fillna('Missing')
    alc_counts = alcohol_data.value_counts()
    colors = ['lightblue', 'skyblue', 'steelblue', 'gray']
    bars = axes[5].bar(range(len(alc_counts)), alc_counts.values, color=colors[:len(alc_counts)])
    axes[5].set_title('Alcohol Intake Distribution', fontsize=12, fontweight='bold')
    axes[5].set_xlabel('Intake Level')
    axes[5].set_ylabel('Count')
    axes[5].set_xticks(range(len(alc_counts)))
    axes[5].set_xticklabels(alc_counts.index, rotation=45, ha='right')

    for bar, count in zip(bars, alc_counts.values):
        height = bar.get_height()
        axes[5].text(bar.get_x() + bar.get_width()/2, height,
                    f'{count:,}', ha='center', va='bottom', fontsize=9)

# Plot 7: Hypertension distribution
if 'Hypertension' in predictor_df.columns:
    ht_counts = predictor_df['Hypertension'].value_counts()
    labels = ['No HTN', 'HTN'] if predictor_df['Hypertension'].nunique() == 2 else ht_counts.index
    axes[6].pie(ht_counts.values, labels=labels, autopct='%1.1f%%',
                colors=['lightgreen', 'salmon'], startangle=90)
    axes[6].set_title('Hypertension Prevalence', fontsize=12, fontweight='bold')

# Plot 8: Diabetes distribution
if 'Diabetes' in predictor_df.columns:
    dm_counts = predictor_df['Diabetes'].value_counts()
    labels = ['No DM', 'DM'] if predictor_df['Diabetes'].nunique() == 2 else dm_counts.index
    axes[7].pie(dm_counts.values, labels=labels, autopct='%1.1f%%',
                colors=['lightblue', 'coral'], startangle=90)
    axes[7].set_title('Diabetes Prevalence', fontsize=12, fontweight='bold')

# Plot 9: Smoking status
if 'Smoking_Status' in predictor_df.columns:
    smoke_counts = predictor_df['Smoking_Status'].value_counts()
    axes[8].bar(range(len(smoke_counts)), smoke_counts.values,
                color=['lightgreen', 'orange', 'red'], alpha=0.7)
    axes[8].set_title('Smoking Status Distribution', fontsize=12, fontweight='bold')
    axes[8].set_xlabel('Smoking Status')
    axes[8].set_ylabel('Count')
    axes[8].set_xticks(range(len(smoke_counts)))
    axes[8].set_xticklabels(smoke_counts.index, rotation=45, ha='right')

    for i, count in enumerate(smoke_counts.values):
        axes[8].text(i, count, f'{count:,}', ha='center', va='bottom', fontsize=9)

plt.tight_layout()
plt.show()

print("\n" + "="*60)
print("KEY FINDINGS:")
print("="*60)

findings = """
1. Genetic Data: CYP2C9 and VKORC1 variants show expected distributions; no missing genetic data.
2. Clinical Variables: Age 40–90, weight 45–120 kg; comorbidities (HTN 40%, Diabetes 20%, CKD 10%); meds (Aspirin/Statins ~30%).
3. Lifestyle: Alcohol intake missing ~40% (needs 'Unknown'); smoking distribution plausible; Vitamin K balanced.
4. Feature Engineering Priorities:
   - Genetic: Encode CYP2C9 (metabolizer), VKORC1 (sensitivity), allele dose.
   - Clinical: BMI, age groups, comorbidity & drug interaction scores.
   - Missing Data: Flag alcohol unknown, consider imputation.
   - Target: Warfarin dose skewed → log transform or categorize (low/med/high).
"""

print(findings)

"""## Correlation & Multicollinearity Analysis"""

print("\n=== CORRELATION & MULTICOLLINEARITY ANALYSIS ===")

# Focus on numerical predictors for correlation analysis
numerical_for_corr = [col for col in numerical_predictors if col != 'Patient_ID']

# Calculate correlation matrix
corr_matrix = predictor_df[numerical_for_corr].corr()

print(f"\nAnalyzing correlations among {len(numerical_for_corr)} numerical predictors...")

# Identify highly correlated pairs (|r| > 0.7)
high_corr_pairs = []
for i in range(len(corr_matrix.columns)):
    for j in range(i+1, len(corr_matrix.columns)):
        corr_value = abs(corr_matrix.iloc[i, j])
        if corr_value > 0.7:
            high_corr_pairs.append((
                corr_matrix.columns[i],
                corr_matrix.columns[j],
                corr_matrix.iloc[i, j]
            ))

print(f"\nHighly correlated pairs (|r| > 0.7): {len(high_corr_pairs)}")

if high_corr_pairs:
    print("\nTop highly correlated pairs:")
    for pair in high_corr_pairs[:5]:  # Show top 5
        print(f"  {pair[0]} ↔ {pair[1]}: r = {pair[2]:.3f}")
else:
    print("  No high correlations detected (good for modeling)")

# Check correlation with target
if 'Final_Stable_Dose_mg' in df.columns:
    target_correlations = {}
    for col in numerical_for_corr:
        if col != 'Final_Stable_Dose_mg':
            corr = df['Final_Stable_Dose_mg'].corr(predictor_df[col])
            target_correlations[col] = abs(corr)

    # Sort by absolute correlation
    sorted_corrs = sorted(target_correlations.items(), key=lambda x: x[1], reverse=True)

    print(f"\nTop 5 predictors correlated with warfarin dose:")
    for col, corr in sorted_corrs[:5]:
        print(f"  {col}: |r| = {corr:.3f}")

# VIF calculation for multicollinearity
print("\n" + "="*60)
print("VARIANCE INFLATION FACTOR (VIF) ANALYSIS")
print("="*60)

from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

# Prepare numeric data (drop missing values)
vif_data = predictor_df[numerical_for_corr].dropna()

if not vif_data.empty:
    vif_data_const = add_constant(vif_data)
    vif_series = pd.Series(
        [variance_inflation_factor(vif_data_const.values, i)
         for i in range(vif_data_const.shape[1])],
        index=vif_data_const.columns
    )

    print("\nVIF values (VIF > 5 indicates multicollinearity):")
    for feature, vif in vif_series.items():
        if feature != 'const':
            print(f"{feature}: VIF = {vif:.2f}")

    # Count problematic features
    high_vif = (vif_series.drop('const') > 5).sum()
    print(f"\nFeatures with VIF > 5: {high_vif}")
else:
    print("Could not calculate VIF due to missing values")

# Visualize correlation matrix (simplified, blue shades)
plt.figure(figsize=(8, 6))   #description: set plot size
sns.heatmap(corr_matrix, cmap="Blues", center=0, annot=False)   #description: heatmap with blue/light blue color scale
plt.title("Correlation Matrix", fontsize=14)   #description: add title
plt.show()   #description: display plot

print("\n" + "="*60)
print("KEY FINDINGS:")
print("="*60)

print("""
1. Multicollinearity: Low overall (VIF ≈ 1). Amiodarone & On_Amiodarone perfectly collinear → remove one.
2. Target Correlations: Weak–moderate; strongest predictors = CYP2C9, VKORC1, Age, Weight, meds.
3. Feature Engineering: Keep all predictors; add interaction terms (Genetics × Age, Genetics × Drugs).
4. Modeling Strategy: Linear regression safe; Tree models (RF, GB) capture nonlinear/interaction effects; use feature importance.
""")

# Drop On_Amiodarone (derived from Amiodarone, perfect correlation)
if 'On_Amiodarone' in predictor_df.columns:
    predictor_df = predictor_df.drop(columns=['On_Amiodarone'])
    print(f"   Removed 'On_Amiodarone' (perfect correlation with 'Amiodarone')")
    print(f"   Kept 'Amiodarone' as original medication variable")

# Recalculate VIF without problematic variable
print(f"   Current predictors: {predictor_df.shape[1]}")

# Create Train-Validation-Test Split
print("\n DATA SPLITTING STRATEGY:")

# Define target
target_col = 'Final_Stable_Dose_mg'

# Separate features and target
X = predictor_df.drop(columns=[target_col], errors='ignore')
y = df[target_col]

print(f"   Features: {X.shape[1]} predictors")
print(f"   Target: {target_col} (range: {y.min():.1f}-{y.max():.1f} mg)")
print(f"   Samples: {X.shape[0]:,} total")

# Split 1: Initial split to separate test set (20%)
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, shuffle=True
)

# Split 2: Split remaining into train and validation (25% of original = 20% of remaining)
X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=0.25, random_state=42, shuffle=True  # 0.25 * 0.8 = 0.2
)

print(f"   Training set: {X_train.shape[0]:,} samples (60%)")
print(f"   Validation set: {X_val.shape[0]:,} samples (20%)")
print(f"   Test set: {X_test.shape[0]:,} samples (20%)")

# Verify splits maintain similar target distribution
split_stats = pd.DataFrame({
    'Set': ['Train', 'Validation', 'Test'],
    'Samples': [len(X_train), len(X_val), len(X_test)],
    'Mean_Dose': [y_train.mean(), y_val.mean(), y_test.mean()],
    'Std_Dose': [y_train.std(), y_val.std(), y_test.std()]
})

print(f"\n   Target distribution across splits:")
print(split_stats.to_string(index=False))

# Save Prepared Datasets
print("\n. SAVING PREPARED DATASETS:")

# Save all splits
X_train.to_csv('X_train_final.csv', index=False)
X_val.to_csv('X_val_final.csv', index=False)
X_test.to_csv('X_test_final.csv', index=False)

y_train.to_csv('y_train_final.csv', index=False)
y_val.to_csv('y_val_final.csv', index=False)
y_test.to_csv('y_test_final.csv', index=False)

print("   Training features: X_train_final.csv")
print("   Validation features: X_val_final.csv")
print("   Test features: X_test_final.csv")
print("   Training target: y_train_final.csv")
print("   Validation target: y_val_final.csv")
print("   Test target: y_test_final.csv")

"""## FOR DOCUMENTATION PURPOSE"""

# Creating Metadata

# Create variable type mapping
variable_types = {}
for col in X_train.columns:
    if col in categorical_predictors:
        variable_types[col] = 'categorical'
    else:
        variable_types[col] = 'numerical'

# Save metadata
metadata = {
    'project': 'Warfarin Precision Dosing',
    'date_prepared': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M'),
    'data_shape': {
        'total_samples': len(df),
        'train_samples': len(X_train),
        'val_samples': len(X_val),
        'test_samples': len(X_test),
        'n_features': X_train.shape[1]
    },
    'target_variable': {
        'name': target_col,
        'type': 'continuous',
        'range_mg': f"{y.min():.1f}-{y.max():.1f}",
        'mean_mg': f"{y.mean():.2f}",
        'std_mg': f"{y.std():.2f}"
    },
    'feature_types': variable_types,
    'excluded_variables': {
        'leakage': ['TTR_pct', 'INR_Stabilization_Days', 'Adverse_Event'],
        'derived': ['Alcohol_Score', 'VitK_Score', 'On_Amiodarone']
    },
    'missing_data_handling': {
        'Alcohol_Intake': 'preserved as separate category',
        'Adverse_Event': 'excluded (outcome variable)'
    }
}

import json
with open('modeling_metadata.json', 'w') as f:
    json.dump(metadata, f, indent=2)

print("   Modeling metadata: modeling_metadata.json")

print("\n MODELING ROADMAP:")

roadmap = f"""
DAY 3 MODEL DEVELOPMENT PLAN
{'='*60}

A. BASELINE MODELS TO IMPLEMENT:
   1. Linear Regression (with regularization)
   2. Random Forest Regressor
   3. Gradient Boosting Regressor (XGBoost/LightGBM)

B. FEATURE ENGINEERING (TO IMPLEMENT):
   1. Genetic encoding:
      - CYP2C9: Convert to metabolizer status (Poor/Intermediate/Normal)
      - VKORC1: Convert to sensitivity score (0,1,2)
   2. Clinical interactions:
      - Age × Genetic variants
      - Weight × CYP2C9 status
      - Drug interaction scores
   3. Missing data flags:
      - Alcohol_Intake_Unknown flag

C. VALIDATION STRATEGY:
   - Training set: Model fitting
   - Validation set: Hyperparameter tuning
   - Test set: Final evaluation only

D. EVALUATION METRICS (PRIMARY):
   - Root Mean Squared Error (RMSE)
   - Mean Absolute Error (MAE)
   - R-squared (R²)

E. FEATURE IMPORTANCE ANALYSIS:
   - Permutation importance
   - SHAP values for interpretability

F. COMPARISON BASELINE:
   - IWPC algorithm (clinical standard)
"""

print(roadmap)

# Save roadmap
with open('day3_modeling_roadmap.txt', 'w') as f:
    f.write(roadmap)

print("   Modeling roadmap: day3_modeling_roadmap.txt")

print("\n FINAL DATA CHECK:")

print(f"   Training set shape: {X_train.shape}")
print(f"   Validation set shape: {X_val.shape}")
print(f"   Test set shape: {X_test.shape}")

# Check for any remaining missing values
missing_check = pd.concat([X_train, X_val, X_test]).isnull().sum()
missing_vars = missing_check[missing_check > 0]

if len(missing_vars) == 0:
    print("   No missing values in feature sets")
else:
    print(f"    Missing values found in: {list(missing_vars.index)}")

# Verify feature types
categorical_count = sum(1 for v in variable_types.values() if v == 'categorical')
numerical_count = sum(1 for v in variable_types.values() if v == 'numerical')

print(f"   Feature types: {categorical_count} categorical, {numerical_count} numerical")